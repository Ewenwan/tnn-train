#
# Sample Configuration File for MLP
#


#
# arch
#
arch.modelArch=MLP  # simple MLP
arch.neuronPerLayerCount.1=1000
arch.neuronPerLayerCount.2=1000
arch.dropout_in=0
arch.dropout_hidden=0
arch.batchnorm.epsilon=0
arch.activationFn=Tanh
arch.batchNorm=1
arch.criterion=CrossEntropyCriterion
arch.criterionArg=
arch.paramInit.x=weight # weight | bias
arch.paramInit.fn=constant              # e.g.xavier full list at https://github.com/Kaixhin/nninit
arch.paramInit.args=0.2

#
# input
#
input.narize.bitCount=1
input.narize.signed=0
input.fractureData=1
input.distort.count=0
input.distort.rotate=0
input.distort.scale=0
input.distort.translate=0
input.dataset=mnist
input.normalize=0
input.scale=0
input.zca=0
input.gcn=0
input.lecunlcn=0
input.cacheOnMiss=0
#
# run
#
run.batchSize=100
run.randseed=1
run.shuffleIndices=1
#
# main
#
main.verbose=3
main.stopEarly=20
main.threads=1
main.cuda=0
main.cudnn=0
main.debug=0
main.dstDir=results_%arch.modelArch%_%main.confhash%
main.binPath=
main.epochCount=3
main.logFileSize=100000000
main.trace.loss=0
main.trace.learningRate=0
main.hist.params=0
#
#
#
# example configuration for ADAM optimization
#
run.optim.fn=adam
run.optim.conf.learningRate=0.01       # learning rate
run.optim.conf.learningRateDecay='epoch%100==0 and lr*0.5 or lr'    # formula to return learning rate, may use 'epoch','batch','lr' variables (1-indexed), evaluated at start of epoch or end of batch (if formula contains 'batch' variable)
run.optim.conf.beta1=0.9                # first moment coefficient
run.optim.conf.beta2=0.999              # second moment coefficient
run.optim.conf.epsilon=1e-8             # for numerical stability
run.optim.conf.weightDecay=0            # weight decay
