#
# example configuration for ADAGRAD optimization
#
run.optim.fn=adagrad
run.optim.conf.learningRate=1e-3        # learning rate
run.optim.conf.learningRateDecay=0      # learning rate decay, as implemented by optim (1/t decay formula)
run.optim.conf.weightDecay=0            # scalar that controls weight decay
