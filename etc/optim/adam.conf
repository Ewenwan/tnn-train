#
# example configuration for ADAM optimization
#
run.optim.fn=adam
run.optim.conf.learningRate=0.01       # learning rate
run.optim.conf.learningRateDecay='epoch%100==0 and lr*0.5 or lr'    # formula to return learning rate, may use 'epoch','batch','lr' variables (1-indexed), evaluated at start of epoch or end of batch (if formula contains 'batch' variable)
run.optim.conf.beta1=0.9                # first moment coefficient
run.optim.conf.beta2=0.999              # second moment coefficient
run.optim.conf.epsilon=1e-8             # for numerical stability
run.optim.conf.weightDecay=0            # weight decay
