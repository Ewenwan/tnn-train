#
# example configuration for ADAMAX optimization
#
run.optim.fn=adamax
run.optim.conf.learningRate=0.002       # learning rate
run.optim.conf.learningRateDecay.100=0.005       # learning rate modified to 0.005 at start of epoch 100
run.optim.conf.learningRateDecay.200=0.001       # learning rate modified to 0.001 at start of epoch 200
run.optim.conf.beta1=0.9                # first moment coefficient
run.optim.conf.beta2=0.999              # second moment coefficient
run.optim.conf.epsilon=1e-38            # for numerical stability
run.optim.conf.weightDecay=0            # weight decay
